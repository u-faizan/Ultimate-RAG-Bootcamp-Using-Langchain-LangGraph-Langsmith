# Section 6: Embeddings

This section focuses on vector embeddings — how text is converted into numerical vectors, model choices, and practical considerations for using embeddings in RAG systems.

## Topics Covered

- Embedding fundamentals and intuition
- Popular embedding models (OpenAI, sentence-transformers, local/embedder models)
- Embedding dimensionality and performance trade-offs
- Batching, normalization, and preprocessing
- Similarity metrics (cosine, dot product) and index choices
- Practical examples using LangChain and OpenAI

## Learning Objectives

- Understand what embeddings represent and why they are essential for retrieval
- Choose appropriate embedding models for prototyping vs production
- Generate embeddings at scale (batching / parallelism)
- Integrate embeddings into vectorstores and retrieval pipelines

## Hands-on Materials

- `embedding.ipynb` — Practical notebook demonstrating local and transformer-based embeddings
- `openaiembeddings.ipynb` — Examples using OpenAI embeddings API
- `18-Embeddings.pdf` — Instructor reference slides and notes

See `notes.md` for implementation notes, best practices, and code snippets.
